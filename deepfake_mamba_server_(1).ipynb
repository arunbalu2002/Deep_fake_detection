{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunbalu2002/Deep_fake_detection/blob/main/deepfake_mamba_server_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mamba-ssm\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "Nll1kDEE6JWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5aad2ca-cabb-4a43-8add-d6172cd17919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.4.tar.gz (91 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/91.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (2.6.0+cu124)\n",
            "Collecting ninja (from mamba-ssm)\n",
            "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (4.50.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->mamba-ssm)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba-ssm) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2025.1.31)\n",
            "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.4-cp311-cp311-linux_x86_64.whl size=323672993 sha256=8a0be01153fa30727a9e69024fbe061eb92c7ba4416d2049c5fc3107ed91d852\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/5e/64/cfcb5dfe4f854944456e031c34953dc872af1ad7c206145d4a\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mamba-ssm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed mamba-ssm-2.2.4 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiLuz11LKS7R",
        "outputId": "6a8431ea-985d-406e-954f-0e11f7b722f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "T51xSNmoio-6",
        "outputId": "38285879-51e6-4dbe-b004-c48dafdea915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9lYEjfo555K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "2c938884-8c30-496b-c2a7-ab588290d544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ddac3403b8d8a65d88.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ddac3403b8d8a65d88.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "from mamba_ssm.models.mixer_seq_simple import Mamba\n",
        "\n",
        "\n",
        "# Add the MetricsTracker class definition to this file\n",
        "class MetricsTracker:\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "        self.best_val_acc = 0\n",
        "        self.epochs_without_improvement = 0\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.train_losses.append(train_loss)\n",
        "        self.train_accuracies.append(train_acc)\n",
        "        self.val_losses.append(val_loss)\n",
        "        self.val_accuracies.append(val_acc)\n",
        "\n",
        "        if val_acc > self.best_val_acc:\n",
        "            self.best_val_acc = val_acc\n",
        "            self.epochs_without_improvement = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.epochs_without_improvement += 1\n",
        "            return False\n",
        "\n",
        "class VisionMambaClassifier(nn.Module):\n",
        "    def __init__(self, image_size=224, patch_size=16, dim=768, depth=24):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize backbone\n",
        "        self.backbone = timm.create_model('efficientnet_b0', pretrained=True, features_only=True)\n",
        "        backbone_channels = self.backbone.feature_info[-1]['num_chs']\n",
        "\n",
        "        # Calculate the output size from backbone\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 3, image_size, image_size)\n",
        "            features = self.backbone(dummy_input)[-1]\n",
        "            feature_size = features.shape[-1]  # Should be 7 for 224x224 input\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (feature_size * feature_size)\n",
        "\n",
        "        # Fixed patch embedding to handle 7x7 feature maps\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv2d(backbone_channels, dim, kernel_size=1),\n",
        "            nn.LayerNorm([dim, feature_size, feature_size]),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, dim))\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Deeper Mamba blocks with skip connections\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(dim),\n",
        "                Mamba(\n",
        "                    d_model=dim,\n",
        "                    d_state=32,\n",
        "                    d_conv=8,\n",
        "                    expand=4\n",
        "                ),\n",
        "                nn.Dropout(0.2)\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(dim, dim // 2),\n",
        "            nn.LayerNorm(dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(dim // 2, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features using EfficientNet backbone\n",
        "        features = self.backbone(x)[-1]\n",
        "\n",
        "        # Process through patch embedding\n",
        "        x = self.patch_embed(features)\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Ensure pos_embed matches the sequence length\n",
        "        if x.size(1) != self.pos_embed.size(1):\n",
        "            x = x + self.pos_embed[:, :x.size(1), :]\n",
        "        else:\n",
        "            x = x + self.pos_embed\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Process through Mamba blocks with residual connections\n",
        "        for block in self.blocks:\n",
        "            x = x + block(x)\n",
        "\n",
        "        # Global average pooling and classification\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.norm(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Global variables\n",
        "model = None\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_model(model_path='/content/drive/MyDrive/Mamba/best_model.pth'):\n",
        "    \"\"\"Load the model from checkpoint\"\"\"\n",
        "    global model\n",
        "    if model is None:\n",
        "        model = VisionMambaClassifier()\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model = model.to(device)\n",
        "            model.eval()\n",
        "            return \"Model loaded successfully!\"\n",
        "        except Exception as e:\n",
        "            return f\"Error loading model: {str(e)}\"\n",
        "    return \"Model already loaded\"\n",
        "\n",
        "def predict_image(image, model_path='/content/drive/MyDrive/vision_mamba/best_model.pth'):\n",
        "    \"\"\"Process the image and make a prediction\"\"\"\n",
        "    if image is None:\n",
        "        return \"No image provided\", None, None, None\n",
        "\n",
        "    # Ensure model is loaded\n",
        "    global model\n",
        "    if model is None:\n",
        "        load_status = load_model(model_path)\n",
        "        if \"Error\" in load_status:\n",
        "            return load_status, None, None, None\n",
        "\n",
        "    try:\n",
        "        # Convert to PIL Image if needed\n",
        "        if not isinstance(image, Image.Image):\n",
        "            image = Image.fromarray(image).convert('RGB')\n",
        "        else:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            prediction = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # Get results\n",
        "        pred_class = 'Real' if prediction.item() == 1 else 'Fake'\n",
        "        probs = probabilities[0].cpu().numpy()\n",
        "\n",
        "        fake_prob = float(probs[0])\n",
        "        real_prob = float(probs[1])\n",
        "\n",
        "        # Create results message\n",
        "        results_msg = f\"Prediction: {pred_class} (Confidence: {max(probs) * 100:.2f}%)\\n\"\n",
        "        results_msg += f\"Fake probability: {fake_prob * 100:.2f}%\\n\"\n",
        "        results_msg += f\"Real probability: {real_prob * 100:.2f}%\"\n",
        "\n",
        "        return results_msg, pred_class, fake_prob, real_prob\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during prediction: {str(e)}\", None, None, None\n",
        "\n",
        "def update_plot(fake_prob, real_prob):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    if fake_prob is None or real_prob is None:\n",
        "        return None\n",
        "\n",
        "    # Create confidence visualization\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    categories = ['Fake', 'Real']\n",
        "    values = [fake_prob * 100, real_prob * 100]\n",
        "    colors = ['#FF5733' if values[0] > values[1] else '#33FF57',\n",
        "             '#33FF57' if values[1] > values[0] else '#FF5733']\n",
        "\n",
        "    ax.barh(categories, values, color=colors)\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_xlabel('Confidence (%)')\n",
        "\n",
        "    for i, v in enumerate(values):\n",
        "        ax.text(v + 1, i, f\"{v:.1f}%\", va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def create_interface():\n",
        "    \"\"\"Create and launch the Gradio interface\"\"\"\n",
        "    with gr.Blocks(title=\"Deepfake Detection\") as interface:\n",
        "        gr.Markdown(\"# Deepfake Detection\")\n",
        "        gr.Markdown(\"Upload an image or take a picture with your camera to check if it's real or AI-generated\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Input methods\n",
        "                with gr.Tab(\"Upload Image\"):\n",
        "                    input_image = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "\n",
        "                with gr.Tab(\"Camera\"):\n",
        "                    # Fixed: Use webcam() method instead of source parameter\n",
        "                    camera_input = gr.Image(label=\"Take Photo\", type=\"pil\")\n",
        "                    camera_button = gr.Button(\"Capture from Webcam\")\n",
        "\n",
        "                # Model path input\n",
        "                model_path = gr.Textbox(label=\"Model Path\", value=\"/content/drive/MyDrive/vision_mamba/best_model.pth\")\n",
        "\n",
        "                # Buttons\n",
        "                with gr.Row():\n",
        "                    load_button = gr.Button(\"Load Model\")\n",
        "                    analyze_button = gr.Button(\"Analyze Image\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                # Results display\n",
        "                result_text = gr.Textbox(label=\"Results\", lines=5)\n",
        "\n",
        "                with gr.Row():\n",
        "                    prediction_label = gr.Label(label=\"Prediction\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    fake_score = gr.Number(label=\"Fake Score\", value=0, interactive=False)\n",
        "                    real_score = gr.Number(label=\"Real Score\", value=0, interactive=False)\n",
        "\n",
        "                # Visual feedback with progress bar\n",
        "                with gr.Row():\n",
        "                    gr.Markdown(\"### Confidence Visualization\")\n",
        "                    fake_bar = gr.Plot(label=\"Confidence Scores\")\n",
        "\n",
        "        # Function to handle input from either upload or camera\n",
        "        def process_input(image, camera_img, model_path):\n",
        "            # Use whichever input is not None\n",
        "            img_to_process = image if image is not None else camera_img\n",
        "            if img_to_process is None:\n",
        "                return \"Please provide an image through upload or camera\", None, None, None, None\n",
        "\n",
        "            result, pred_class, fake_prob, real_prob = predict_image(img_to_process, model_path)\n",
        "\n",
        "            # Create confidence plot\n",
        "            plot = update_plot(fake_prob, real_prob)\n",
        "\n",
        "            # Create Label output\n",
        "            label_output = {pred_class: max(fake_prob, real_prob)} if pred_class else None\n",
        "\n",
        "            return result, label_output, fake_prob, real_prob, plot\n",
        "\n",
        "        # Connect functions to events\n",
        "        load_button.click(fn=load_model, inputs=[model_path], outputs=[result_text])\n",
        "\n",
        "        # Make the camera button use the webcam\n",
        "        if hasattr(camera_input, 'webcam'):\n",
        "            # For newer versions of Gradio\n",
        "            camera_button.click(fn=lambda: None, inputs=None, outputs=None, _js=\"() => {document.querySelector('button.webcam-trigger').click(); return null;}\")\n",
        "\n",
        "        analyze_button.click(\n",
        "            fn=process_input,\n",
        "            inputs=[input_image, camera_input, model_path],\n",
        "            outputs=[result_text, prediction_label, fake_score, real_score, fake_bar]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Launch the interface if run directly\n",
        "if __name__ == \"__main__\":\n",
        "    interface = create_interface()\n",
        "    interface.launch(share=True)"
      ]
    }
  ]
}